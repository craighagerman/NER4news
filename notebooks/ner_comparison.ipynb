{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of NER frameworks/approaches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\"\n",
    "\n",
    "# Huggingface datasets is needed to load the CONLL 2003 data\n",
    "from datasets import load_dataset, Dataset\n",
    "# A Hugging library for easily evaluating machine learning models and datasets.\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CONLL 2003 dataset\n",
    "conll = load_dataset(\"conll2003\")\n",
    "tag_names = conll[\"test\"].features[f\"ner_tags\"].feature.names\n",
    "test = conll[\"test\"]\n",
    "\n",
    "tag_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pre-trained BERT model to predict NER labels\n",
    "\n",
    "\n",
    "see `CAVEAT` cell below\n",
    "- This cell will download a large (400+MB) pre-trained model and likely take several minutes to do so.\n",
    "- The Bert model will be saved to the local filesystem and not downloaded on subsequent invocations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed dependences\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from transformers.pipelines.token_classification import TokenClassificationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "transformers.pipelines.token_classification.TokenClassificationPipeline"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CAVEAT: Long-running cell on initial invocation\n",
    "\n",
    "# load pre-trained BERT base cased model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "bert_model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "bert_nlp: TokenClassificationPipeline = pipeline(\"ner\", model=bert_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = test.select(range(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_bert(result: int) -> str:\n",
    "    \"\"\" Helper function to tranlate NER int to class label \"\"\"\n",
    "    if len(result) == 0:\n",
    "        return 'O'\n",
    "    return result[0]['entity']\n",
    "\n",
    "def predict_ner_bert(bert_nlp: TokenClassificationPipeline, labeled_dataset: Dataset) -> Tuple[list[list[str]], list[list[str]]]:\n",
    "    \"\"\" Run inference on the tokens using trained BERT model \"\"\"\n",
    "    references: list[list[str]] = []\n",
    "    predictions: list[list[str]] = []\n",
    "\n",
    "    for row in tqdm(labeled_dataset, desc=str(len(labeled_dataset))):\n",
    "        # add ground truth labels to references\n",
    "        references.append([tag_names[id] for id in row['ner_tags']])\n",
    "        # recognize named entity in a test tokens\n",
    "        ner_results = bert_nlp(row['tokens'])\n",
    "        # translate numerical index to NER class label\n",
    "        predicted_tags = [annotate_bert(x) for x in ner_results]\n",
    "        predictions.append(predicted_tags)\n",
    "    return references, predictions\n",
    "\n",
    "########################################\n",
    "# I/O Helper Functions\n",
    "########################################\n",
    "def _load_persisted_json(inpath: str) -> dict:\n",
    "    return json.loads(open(inpath).read())\n",
    "\n",
    "def save_ner_results(outpath: str, references: list[list[str]], predictions: list[list[str]]) -> None:\n",
    "    \"\"\" Helper function for persisting true and predicted NER labels \"\"\"\n",
    "    print(f\"Saving NER results to {outpath}\")\n",
    "    d = {\"references\": references, \"predictions\": predictions}\n",
    "    with open(outpath, \"w\") as fo:\n",
    "        fo.write(json.dumps(d))\n",
    "\n",
    "def load_ner_results(inpath: str) -> Tuple[list[list[str]], list[list[str]]]:\n",
    "    \"\"\" Helper function for loading previously persisted true and predicted NER labels \"\"\"\n",
    "    d = json.loads(open(inpath).read())\n",
    "    return d[\"references\"], d[\"predictions\"]\n",
    "\n",
    "\n",
    "def evaluate_results(references: list[list[str]], predictions: list[list[str]]):\n",
    "    seqeval = evaluate.load(\"seqeval\")\n",
    "    return seqeval.compute(predictions=predictions, references=references)\n",
    "\n",
    "def save_evaluation_results(outpath: str, results: dict) -> None:\n",
    "    print(f\"Saving evaluation results to {outpath}\")\n",
    "    with open(outpath, \"w\") as fo:\n",
    "        # fo.write(json.dumps(results))\n",
    "        fo.write(json.dumps(results, indent=2, default=float))\n",
    "\n",
    "def load_evaluation_results(inpath: str) -> dict:\n",
    "    return _load_persisted_json(inpath)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10: 100%|██████████| 10/10 [00:27<00:00,  2.77s/it]\n"
     ]
    }
   ],
   "source": [
    "# Generate a list of ground truth NER labels and predictions\n",
    "references, bert_predictions = predict_ner_bert(bert_nlp, test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving NER results to /Users/chagerman/MyProjects/NER4news/data/interim/ner_results_bert.json\n"
     ]
    }
   ],
   "source": [
    "# Save NER results to disk\n",
    "interim_dir = \"/Users/chagerman/MyProjects/NER4news/data/interim\"\n",
    "bert_results_path = os.path.join(interim_dir, \"ner_results_bert.json\")\n",
    "\n",
    "save_ner_results(bert_results_path, references, bert_predictions)\n",
    "\n",
    "# Load persisted NER results\n",
    "# references, bert_predictions = load_ner_results(bert_results_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving evaluation results to /Users/chagerman/MyProjects/NER4news/data/interim/evaluation_results_bert.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = evaluate_results(references, bert_predictions)\n",
    "\n",
    "bert_evaluation_path = os.path.join(interim_dir, \"evaluation_results_bert.json\")\n",
    "save_evaluation_results(bert_evaluation_path, results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_lg = spacy.load(\"en_core_web_lg\")\n",
    "nlp_trf = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_dataset = test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "archive_dir = \"/Users/chagerman/MyProjects/NER4news/data/external/archive\"\n",
    "test_data_path = os.path.join(archive_dir, \"test.txt\")\n",
    "spacy_test_data_path = os.path.join(archive_dir, \"test_spacy.txt\")\n",
    "test_data = [x.strip() for x in open(test_data_path)]\n",
    "\n",
    "def process_line(line):\n",
    "    return re.sub(\"([BI])-(PER)\", r\"\\g<1>-PERSON\", line )\n",
    "\n",
    "test_data = [process_line(line) for line in test_data]\n",
    "\n",
    "with open(spacy_test_data_path, \"w\") as fo:\n",
    "    fo.write(\"\\n\".join(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd Users/chagerman/MyProjects/NER4news/data/external/archive\n",
    "python -m spacy convert \"test_spacy.txt\" spacyNER_data -c ner\n",
    "python -m spacy evaluate en_core_web_lg spacyNER_data/test.spacy\n",
    "\n",
    "python -m spacy evaluate en_core_web_lg spacyNER_data/test_spacy.spacy > spacy_lg_results.txt\n",
    "python -m spacy evaluate en_core_web_trf spacyNER_data/test_spacy.spacy > spacy_trf_results.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_data_path = \"/Users/chagerman/MyProjects/NER4news/data/external/archive/spacyNER_data/test.spacy\"\n",
    "\n",
    "# nlp_lg.from_disk(spacy_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# text = \"When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously.\"\n",
    "# doc = nlp_lg(text)\n",
    "# for ent in doc.ents:\n",
    "#     print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford NER tagger w. NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "stanford_dir = \"/Users/chagerman/MyProjects/NER4news/ner4news/stanford-ner-2015-04-20\"\n",
    "jar = os.path.join(stanford_dir, \"stanford-ner-3.5.2.jar\")\n",
    "\n",
    "stanford_model_path = os.path.join(stanford_dir, \"classifiers/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_dataset = test.select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanford_model = f\"{stanford_model_path}english.conll.4class.distsim.crf.ser.gz\" \n",
    "stanford_tagger: StanfordNERTagger = StanfordNERTagger(stanford_model, jar, encoding=\"utf8\")\n",
    "\n",
    "def annotate_stanford(result: str) -> str:\n",
    "    \"\"\" Helper function to tranlate NER int to class label \"\"\"\n",
    "    match result:\n",
    "        case \"LOCATION\":\n",
    "            return \"LOC\"\n",
    "        case \"PERSON\":\n",
    "            return \"PER\"\n",
    "        case \"ORGANIZATION\":\n",
    "            return \"ORG\"\n",
    "        case \"MISC\":\n",
    "            return \"MISC\"\n",
    "        case \"O\":\n",
    "            return \"O\"\n",
    "        case _:\n",
    "            return \"X\"\n",
    "            \n",
    "def predict_ner_stanford(stanford_tagger: StanfordNERTagger, labeled_dataset: Dataset) -> Tuple[list[list[str]], list[list[str]]]:\n",
    "    \"\"\" Run inference on the tokens using trained BERT model \"\"\"\n",
    "    references: list[list[str]] = []\n",
    "    st_predictions: list[list[str]] = []\n",
    "\n",
    "    for row in tqdm(labeled_dataset, desc=str(len(labeled_dataset))):\n",
    "        # add ground truth labels to references\n",
    "        references.append([re.sub(\"^[BI]-\", \"\", tag_names[id]) for id in row['ner_tags']])\n",
    "        # recognize named entity in a test tokens\n",
    "        ner_results = stanford_tagger.tag(row['tokens'])\n",
    "        # translate numerical index to NER class label\n",
    "        predicted_tags = [annotate_stanford(y) for x, y in ner_results]\n",
    "        st_predictions.append(predicted_tags)\n",
    "    return references, st_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10: 100%|██████████| 10/10 [00:43<00:00,  4.40s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run NER inference using Stanford NER tagger\n",
    "references, st_predictions = predict_ner_stanford(stanford_tagger, labeled_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ER': {'precision': 1.0,\n",
       "  'recall': 0.75,\n",
       "  'f1': 0.8571428571428571,\n",
       "  'number': 4},\n",
       " 'ISC': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 6},\n",
       " 'OC': {'precision': 0.9166666666666666,\n",
       "  'recall': 1.0,\n",
       "  'f1': 0.9565217391304348,\n",
       "  'number': 11},\n",
       " 'overall_precision': 0.9523809523809523,\n",
       " 'overall_recall': 0.9523809523809523,\n",
       " 'overall_f1': 0.9523809523809523,\n",
       " 'overall_accuracy': 0.9949494949494949}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = evaluate_results(references, st_predictions)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER using Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.nn import Classifier\n",
    "from flair.models import SequenceTagger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-28 17:33:59,275 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# load the 4-class NER tagger\n",
    "# tagger = Classifier.load(\"ner\")\n",
    "# tagger = SequenceTagger.load(\"flair/ner-english\")\n",
    "flair_tagger = Classifier.load('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nadim Ladki / PER  idx: 0\n",
      "Nadim Ladki / PER  idx: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['PER', 'PER']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample\n",
    "\n",
    "text = \"Nadim Ladki\"\n",
    "sentence = Sentence(text)\n",
    "# run NER over sentence\n",
    "flair_tagger.predict(sentence)\n",
    "\n",
    "fl_preds = [\"O\" for _ in range(len(text.split()))]\n",
    "\n",
    "for entity in sentence.get_spans('ner'):\n",
    "    tag = entity.tag\n",
    "    for token in entity:\n",
    "        idx = token.idx - 1\n",
    "        print(f\"{entity.text} / {tag}  idx: {idx}\")\n",
    "        fl_preds[idx] = tag   \n",
    "\n",
    "fl_preds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAPAN / LOC  idx: 3\n",
      "CHINA / LOC  idx: 8\n",
      "['SOCCER', '-', 'JAPAN', 'GET', 'LUCKY', 'WIN', ',', 'CHINA', 'IN', 'SURPRISE', 'DEFEAT', '.']\n",
      "['O', 'O', 'O', 'LOC', 'O', 'O', 'O', 'O', 'LOC', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "tokens = labeled_dataset.select(range(1))[\"tokens\"][0]\n",
    "sample = \" \".join(tokens)\n",
    "# make a flair sentence object\n",
    "sentence = Sentence(sample)\n",
    "# run NER over sentence\n",
    "flair_tagger.predict(sentence)\n",
    "\n",
    "fl_predictions = [\"O\" for _ in range(len(tokens))]\n",
    "\n",
    "for entity in sentence.get_spans('ner'):\n",
    "    tag = entity.tag\n",
    "    for token in entity:\n",
    "        idx = token.idx - 1\n",
    "        # print(f\"{entity.text} / {tag}  idx: {idx}\")\n",
    "        fl_predictions[idx] = tag   \n",
    "\n",
    "print(tokens)\n",
    "print(fl_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ner_flair(flair_tagger: SequenceTagger, labeled_dataset: Dataset) -> Tuple[list[list[str]], list[list[str]]]:\n",
    "    \"\"\" Run inference on the tokens using trained BERT model \"\"\"\n",
    "    references: list[list[str]] = []\n",
    "    fl_predictions: list[list[str]] = []\n",
    "\n",
    "    for row in tqdm(labeled_dataset, desc=str(len(labeled_dataset))):\n",
    "        # add ground truth labels to references\n",
    "        references.append([re.sub(\"^[BI]-\", \"\", tag_names[id]) for id in row['ner_tags']])\n",
    "        tokens = row[\"tokens\"]\n",
    "        # make a flair sentence object\n",
    "        sentence = Sentence(\" \".join(tokens))\n",
    "        # run NER over sentence\n",
    "        flair_tagger.predict(sentence)\n",
    "        ner_results = [\"O\" for _ in range(len(tokens))]\n",
    "        # print(sentence.get_spans(\"ner\"))\n",
    "        for entity in sentence.get_spans('ner'):\n",
    "            tag = entity.tag\n",
    "            for token in entity:\n",
    "                idx = token.idx - 1\n",
    "                ner_results[idx] = tag   \n",
    "        # translate numerical index to NER class label\n",
    "        predicted_tags = ner_results\n",
    "        fl_predictions.append(predicted_tags)\n",
    "    return references, fl_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5: 100%|██████████| 5/5 [00:06<00:00,  1.25s/it]\n"
     ]
    }
   ],
   "source": [
    "references, fl_predictions = predict_ner_flair(flair_tagger, labeled_dataset.select(range(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ER': {'precision': 1.0,\n",
       "  'recall': 0.5,\n",
       "  'f1': 0.6666666666666666,\n",
       "  'number': 2},\n",
       " 'ISC': {'precision': 0.5,\n",
       "  'recall': 1.0,\n",
       "  'f1': 0.6666666666666666,\n",
       "  'number': 1},\n",
       " 'OC': {'precision': 0.875,\n",
       "  'recall': 1.0,\n",
       "  'f1': 0.9333333333333333,\n",
       "  'number': 7},\n",
       " 'overall_precision': 0.8181818181818182,\n",
       " 'overall_recall': 0.9,\n",
       " 'overall_f1': 0.8571428571428572,\n",
       " 'overall_accuracy': 0.9571428571428572}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = evaluate_results(references, fl_predictions)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
